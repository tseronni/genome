{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-02T22:47:49.489371700Z",
     "start_time": "2023-11-02T22:47:22.907819700Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from src.utils.UsefulPaths import Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "paths = Paths()\n",
    "\n",
    "with open(paths.json_subsectors, 'r') as file:\n",
    "    subsectors = json.load(file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T22:49:20.079878600Z",
     "start_time": "2023-11-02T22:49:20.075865800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "df_abstract = pd.read_parquet(paths.raw_parquet_abstract)\n",
    "\n",
    "offset_div = 50\n",
    "df_joao = df_abstract.iloc[:offset_div].copy().reset_index(drop='index')\n",
    "df_leo = df_abstract.iloc[offset_div:offset_div*2].copy().reset_index(drop='index')\n",
    "df_maxm = df_abstract.iloc[offset_div*2:offset_div*3].copy().reset_index(drop='index')\n",
    "df_rafael = df_abstract.iloc[offset_div*4:offset_div*5].copy().reset_index(drop='index')\n",
    "df_thiago = df_abstract.iloc[offset_div*6:offset_div*7].copy().reset_index(drop='index')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T22:48:40.262221400Z",
     "start_time": "2023-08-31T22:48:33.951792100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 20:31:21,209 - INFO - Load pretrained SentenceTransformer: roberta-base-squad2\n",
      "2023-08-31 20:31:21,383 - DEBUG - https://huggingface.co:443 \"GET /api/models/sentence-transformers/roberta-base-squad2 HTTP/1.1\" 401 41\n"
     ]
    },
    {
     "ename": "RepositoryNotFoundError",
     "evalue": "401 Client Error. (Request ID: Root=1-64f122c7-39387d992bdcd9fe59ad28b3;fdf794c6-1ddc-4525-b64d-34706e6b010e)\n\nRepository Not Found for url: https://huggingface.co/api/models/sentence-transformers/roberta-base-squad2.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[1;32m~\\PycharmProjects\\Genome\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:261\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[1;34m(response, endpoint_name)\u001B[0m\n\u001B[0;32m    260\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 261\u001B[0m     \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    262\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\PycharmProjects\\Genome\\venv\\lib\\site-packages\\requests\\models.py:1021\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1020\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[1;32m-> 1021\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[1;31mHTTPError\u001B[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models/sentence-transformers/roberta-base-squad2",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mRepositoryNotFoundError\u001B[0m                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 134\u001B[0m\n\u001B[0;32m    128\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m df_all\n\u001B[0;32m    130\u001B[0m \u001B[38;5;66;03m# 'bert-base-uncased'\u001B[39;00m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;66;03m# bert-large-uncased-whole-word-masking\u001B[39;00m\n\u001B[0;32m    132\u001B[0m \u001B[38;5;66;03m# bert-base-multilingual-cased\u001B[39;00m\n\u001B[0;32m    133\u001B[0m \u001B[38;5;66;03m# 'all-mpnet-base-v2'\u001B[39;00m\n\u001B[1;32m--> 134\u001B[0m bert_sim \u001B[38;5;241m=\u001B[39m \u001B[43mPatentSimilarity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_patents\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdf_joao\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mroberta-base-squad2\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdoes_include\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdoes_not_include\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    135\u001B[0m df_all_sim \u001B[38;5;241m=\u001B[39m bert_sim\u001B[38;5;241m.\u001B[39msimilarity_between_all_patents_with_all_subsetors_query(break_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m)\n\u001B[0;32m    136\u001B[0m \u001B[38;5;28mprint\u001B[39m(df_all_sim[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpublication_number\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpatent_abstract\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mscore_max_1\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mscore_max_2\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mscore_max_3\u001B[39m\u001B[38;5;124m'\u001B[39m]]\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m50\u001B[39m))\n",
      "Cell \u001B[1;32mIn[23], line 13\u001B[0m, in \u001B[0;36mPatentSimilarity.__init__\u001B[1;34m(self, df_patents, model_name, definition_weight, keywords_weight, does_include, does_not_include, show_progress_bar, top_scores)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m      3\u001B[0m              df_patents: pd\u001B[38;5;241m.\u001B[39mDataFrame \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(),\n\u001B[0;32m      4\u001B[0m              model_name: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbert-base-uncased\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      9\u001B[0m              show_progress_bar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m     10\u001B[0m              top_scores\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m):\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf_patents \u001B[38;5;241m=\u001B[39m df_patents\n\u001B[1;32m---> 13\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m \u001B[43mSentenceTransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;66;03m# Peso para a definição\u001B[39;00m\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefinition_weight \u001B[38;5;241m=\u001B[39m definition_weight\n",
      "File \u001B[1;32m~\\PycharmProjects\\Genome\\venv\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:87\u001B[0m, in \u001B[0;36mSentenceTransformer.__init__\u001B[1;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001B[0m\n\u001B[0;32m     83\u001B[0m     model_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(cache_folder, model_name_or_path\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m     85\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(model_path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodules.json\u001B[39m\u001B[38;5;124m'\u001B[39m)):\n\u001B[0;32m     86\u001B[0m         \u001B[38;5;66;03m# Download from hub with caching\u001B[39;00m\n\u001B[1;32m---> 87\u001B[0m         \u001B[43msnapshot_download\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     88\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     89\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mlibrary_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msentence-transformers\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     90\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mlibrary_version\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m__version__\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     91\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mignore_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mflax_model.msgpack\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrust_model.ot\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtf_model.h5\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     92\u001B[0m \u001B[43m                            \u001B[49m\u001B[43muse_auth_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_auth_token\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     94\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(model_path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodules.json\u001B[39m\u001B[38;5;124m'\u001B[39m)):    \u001B[38;5;66;03m#Load as SentenceTransformer model\u001B[39;00m\n\u001B[0;32m     95\u001B[0m     modules \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_load_sbert_model(model_path)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Genome\\venv\\lib\\site-packages\\sentence_transformers\\util.py:442\u001B[0m, in \u001B[0;36msnapshot_download\u001B[1;34m(repo_id, revision, cache_dir, library_name, library_version, user_agent, ignore_files, use_auth_token)\u001B[0m\n\u001B[0;32m    439\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m use_auth_token:\n\u001B[0;32m    440\u001B[0m     token \u001B[38;5;241m=\u001B[39m HfFolder\u001B[38;5;241m.\u001B[39mget_token()\n\u001B[1;32m--> 442\u001B[0m model_info \u001B[38;5;241m=\u001B[39m \u001B[43m_api\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel_info\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    444\u001B[0m storage_folder \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\n\u001B[0;32m    445\u001B[0m     cache_dir, repo_id\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    446\u001B[0m )\n\u001B[0;32m    448\u001B[0m all_files \u001B[38;5;241m=\u001B[39m model_info\u001B[38;5;241m.\u001B[39msiblings\n",
      "File \u001B[1;32m~\\PycharmProjects\\Genome\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m check_use_auth_token:\n\u001B[0;32m    116\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m--> 118\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Genome\\venv\\lib\\site-packages\\huggingface_hub\\hf_api.py:1678\u001B[0m, in \u001B[0;36mHfApi.model_info\u001B[1;34m(self, repo_id, revision, timeout, securityStatus, files_metadata, token)\u001B[0m\n\u001B[0;32m   1676\u001B[0m     params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mblobs\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   1677\u001B[0m r \u001B[38;5;241m=\u001B[39m get_session()\u001B[38;5;241m.\u001B[39mget(path, headers\u001B[38;5;241m=\u001B[39mheaders, timeout\u001B[38;5;241m=\u001B[39mtimeout, params\u001B[38;5;241m=\u001B[39mparams)\n\u001B[1;32m-> 1678\u001B[0m \u001B[43mhf_raise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1679\u001B[0m d \u001B[38;5;241m=\u001B[39m r\u001B[38;5;241m.\u001B[39mjson()\n\u001B[0;32m   1680\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ModelInfo(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39md)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Genome\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:293\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[1;34m(response, endpoint_name)\u001B[0m\n\u001B[0;32m    279\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m error_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRepoNotFound\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m401\u001B[39m:\n\u001B[0;32m    280\u001B[0m     \u001B[38;5;66;03m# 401 is misleading as it is returned for:\u001B[39;00m\n\u001B[0;32m    281\u001B[0m     \u001B[38;5;66;03m#    - private and gated repos if user is not authenticated\u001B[39;00m\n\u001B[0;32m    282\u001B[0m     \u001B[38;5;66;03m#    - missing repos\u001B[39;00m\n\u001B[0;32m    283\u001B[0m     \u001B[38;5;66;03m# => for now, we process them as `RepoNotFound` anyway.\u001B[39;00m\n\u001B[0;32m    284\u001B[0m     \u001B[38;5;66;03m# See https://gist.github.com/Wauplin/46c27ad266b15998ce56a6603796f0b9\u001B[39;00m\n\u001B[0;32m    285\u001B[0m     message \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    286\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Client Error.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    287\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    291\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m make sure you are authenticated.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    292\u001B[0m     )\n\u001B[1;32m--> 293\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m RepositoryNotFoundError(message, response) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    295\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m400\u001B[39m:\n\u001B[0;32m    296\u001B[0m     message \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    297\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mBad request for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mendpoint_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m endpoint:\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m endpoint_name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mBad request:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    298\u001B[0m     )\n",
      "\u001B[1;31mRepositoryNotFoundError\u001B[0m: 401 Client Error. (Request ID: Root=1-64f122c7-39387d992bdcd9fe59ad28b3;fdf794c6-1ddc-4525-b64d-34706e6b010e)\n\nRepository Not Found for url: https://huggingface.co/api/models/sentence-transformers/roberta-base-squad2.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password."
     ]
    }
   ],
   "source": [
    "class PatentSimilarity:\n",
    "    def __init__(self,\n",
    "                 df_patents: pd.DataFrame = pd.DataFrame(),\n",
    "                 model_name: str = 'bert-base-uncased',\n",
    "                 definition_weight: float = 1,\n",
    "                 keywords_weight: float = 1,\n",
    "                 does_include: float = 1,\n",
    "                 does_not_include: float = -1,\n",
    "                 show_progress_bar=False,\n",
    "                 top_scores=3):\n",
    "\n",
    "        self.df_patents = df_patents\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "        # Peso para a definição\n",
    "        self.definition_weight = definition_weight\n",
    "\n",
    "        # Peso para as keywords\n",
    "        self.keywords_weight = keywords_weight\n",
    "\n",
    "        # Peso para as Does Include\n",
    "        self.does_include = does_include\n",
    "\n",
    "        # Peso para as Does Not Include\n",
    "        self.does_not_include = does_not_include\n",
    "\n",
    "        self.show_progress_bar = show_progress_bar\n",
    "\n",
    "        self.top_scores = top_scores\n",
    "\n",
    "        self.subsetors_embeddings = {}\n",
    "        self.subsetors_definitions = []\n",
    "        for subsetor_name, subsetor_values in subsectors.items():\n",
    "\n",
    "            subsetor_definition = subsetor_values.get('Definition', '')\n",
    "            subsetor_keywords = subsetor_values.get('Keywords', '')\n",
    "            subsetor_does_include = subsetor_values.get('Does include', '')\n",
    "            subsetor_does_not_include = subsetor_values.get('Does not include', '')\n",
    "\n",
    "            self.subsetors_definitions.append(f'{subsetor_keywords}')\n",
    "            # self.subsetors_definitions.append(f'{subsetor_definition}. {subsetor_keywords}.')\n",
    "            # self.subsetors_definitions.append(f'{subsetor_definition}. {subsetor_keywords}. {subsetor_does_include}')\n",
    "\n",
    "            subsetor_definition_emb = self.model.encode(subsetor_definition, convert_to_tensor=True, show_progress_bar=self.show_progress_bar) * self.definition_weight\n",
    "            subsetor_keywords_emb = self.model.encode(subsetor_keywords, convert_to_tensor=True, show_progress_bar=self.show_progress_bar) * self.keywords_weight\n",
    "            subsetor_does_include_emb = self.model.encode(subsetor_does_include, convert_to_tensor=True, show_progress_bar=self.show_progress_bar) * self.does_include\n",
    "            subsetor_does_not_include_emb = self.model.encode(subsetor_does_not_include, convert_to_tensor=True, show_progress_bar=self.show_progress_bar) * self.does_not_include\n",
    "\n",
    "            self.subsetors_embeddings[subsetor_name] = [subsetor_definition_emb, subsetor_keywords_emb, subsetor_does_include_emb, subsetor_does_not_include_emb]\n",
    "\n",
    "    def similarity_between_one_patent_with_one_subsetor(self, patent_abstract, subsetor_name):\n",
    "\n",
    "        # We pass the convert_to_tensor=True parameter to the encode function. This will return a pytorch tensor containing our embeddings. We can then call util.cos_sim(A, B) which computes the cosine similarity between all vectors in A and all vectors in B.\n",
    "\n",
    "        # query_embedding = f'In this patent abstract \"{patent_abstract}\", what is the subsetor definiton that best describes this patent?'\n",
    "        query_embedding = f'In this patent abstract \"{patent_abstract}\", what are the keywords that best describes this patent?'\n",
    "        patent_abstract_emb = self.model.encode(query_embedding, convert_to_tensor=True, show_progress_bar=self.show_progress_bar, normalize_embeddings=False)\n",
    "\n",
    "        subsetor_definition_emb = self.subsetors_embeddings[subsetor_name][0]\n",
    "        subsetor_keywords_emb = self.subsetors_embeddings[subsetor_name][1]\n",
    "        subsetor_does_include_emb = self.subsetors_embeddings[subsetor_name][2]\n",
    "        subsetor_does_not_include_emb = self.subsetors_embeddings[subsetor_name][3]\n",
    "\n",
    "        # Combine os embeddings\n",
    "        # combined_emb = subsetor_definition_emb + subsetor_keywords_emb + subsetor_does_include_emb - subsetor_does_not_include_emb\n",
    "\n",
    "        similarity = util.cos_sim(patent_abstract_emb, subsetor_definition_emb).item()\n",
    "\n",
    "    def similarity_between_one_patent_with_all_subsetors(self, publication_number, patent_abstract):\n",
    "        col_names = ['publication_number', 'patent_abstract']\n",
    "        patents_similarities = [publication_number, patent_abstract]\n",
    "        for subsetor_name, embeddings in self.subsetors_embeddings.items():\n",
    "            col_names.append(subsetor_name)\n",
    "            specific_sim  = self.similarity_between_one_patent_with_one_subsetor(patent_abstract, subsetor_name)\n",
    "            patents_similarities.append(specific_sim)\n",
    "\n",
    "        df_one_patent = pd.DataFrame([patents_similarities], columns=col_names)\n",
    "        return df_one_patent\n",
    "\n",
    "    def similarity_between_all_patents_with_all_subsetors(self, break_index=-1):\n",
    "        df_all = pd.DataFrame()\n",
    "        for index, row in self.df_patents.iterrows():\n",
    "            row_sim = self.similarity_between_one_patent_with_all_subsetors(row['publication_number'], row['abstract'])\n",
    "            df_all = pd.concat([df_all, row_sim])\n",
    "            if break_index == index:\n",
    "                break\n",
    "\n",
    "        df_all['score_max'] = df_all.iloc[: , 2:].max(axis=1)\n",
    "        df_all['score_name'] = df_all.iloc[: , 2:].idxmax(axis=1)\n",
    "\n",
    "        return df_all\n",
    "\n",
    "    def similarity_between_one_patent_with_all_subsetors_query(self, publication_number, patent_abstract):\n",
    "        col_names = ['publication_number', 'patent_abstract']\n",
    "        patents_similarities = [publication_number, patent_abstract]\n",
    "\n",
    "        query = f'In this patent abstract \"{patent_abstract}\", what is the subsetor definiton that best describes this patent?'\n",
    "        query_emb = self.model.encode(query, convert_to_tensor=True, show_progress_bar=self.show_progress_bar, normalize_embeddings=False)\n",
    "\n",
    "        setor_definitions_emb = self.model.encode(self.subsetors_definitions, convert_to_tensor=True, show_progress_bar=self.show_progress_bar, normalize_embeddings=False)\n",
    "        cos_scores = util.cos_sim(query_emb, setor_definitions_emb)\n",
    "\n",
    "        i = 0\n",
    "        for subsetor_name, subsetor_values in subsectors.items():\n",
    "            col_names.append(subsetor_name)\n",
    "            patents_similarities.append(cos_scores[0][i].item())\n",
    "            i = i + 1\n",
    "\n",
    "        df_one_patent = pd.DataFrame([patents_similarities], columns=col_names)\n",
    "        df_one_patent_sorted = df_one_patent.iloc[0, 2:].to_frame().T.sort_values(by=0, axis=1, ascending=False)\n",
    "\n",
    "        for i in range(0, self.top_scores):\n",
    "            col = f'score_max_{i + 1}'\n",
    "            max_col_name = df_one_patent_sorted.columns[i]\n",
    "            max_value = df_one_patent_sorted.iloc[0, i]\n",
    "            df_one_patent[col] = f'{max_col_name} - {max_value}'\n",
    "\n",
    "        return df_one_patent\n",
    "\n",
    "    def similarity_between_all_patents_with_all_subsetors_query(self, break_index=-1):\n",
    "        df_all = pd.DataFrame()\n",
    "        for index, row in self.df_patents.iterrows():\n",
    "            row_sim = self.similarity_between_one_patent_with_all_subsetors_query(row['publication_number'], row['abstract'])\n",
    "            df_all = pd.concat([df_all, row_sim])\n",
    "            if break_index == index:\n",
    "                break\n",
    "\n",
    "        return df_all\n",
    "\n",
    "# 'bert-base-uncased'\n",
    "# bert-large-uncased-whole-word-masking\n",
    "# bert-base-multilingual-cased\n",
    "# 'all-mpnet-base-v2'\n",
    "bert_sim = PatentSimilarity(df_patents=df_joao, model_name='roberta-base-squad2', does_include=1, does_not_include=-1)\n",
    "df_all_sim = bert_sim.similarity_between_all_patents_with_all_subsetors_query(break_index=50)\n",
    "print(df_all_sim[['publication_number', 'patent_abstract', 'score_max_1', 'score_max_2', 'score_max_3']].head(50))\n",
    "df_all_sim[['publication_number', 'patent_abstract', 'score_max_1', 'score_max_2', 'score_max_3']].to_excel(f'C:\\\\Users\\\\thiag\\\\PycharmProjects\\\\genome\\\\data\\\\processed\\\\class_patent.xlsx', index=False, header=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T23:31:21.509017200Z",
     "start_time": "2023-08-31T23:31:21.209730500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 14:41:21,090 - INFO - Load pretrained SentenceTransformer: bert-large-uncased-whole-word-masking\n",
      "2023-08-31 14:41:21,341 - DEBUG - https://huggingface.co:443 \"GET /api/models/bert-large-uncased-whole-word-masking HTTP/1.1\" 200 2227\n",
      "2023-08-31 14:41:21,517 - DEBUG - https://huggingface.co:443 \"HEAD /bert-large-uncased-whole-word-masking/resolve/8b35f05561d0d917087166b71d3c2c83a39104b1/.gitattributes HTTP/1.1\" 200 0\n",
      "2023-08-31 14:41:21,680 - DEBUG - https://huggingface.co:443 \"HEAD /bert-large-uncased-whole-word-masking/resolve/8b35f05561d0d917087166b71d3c2c83a39104b1/README.md HTTP/1.1\" 200 0\n",
      "2023-08-31 14:41:21,844 - DEBUG - https://huggingface.co:443 \"HEAD /bert-large-uncased-whole-word-masking/resolve/8b35f05561d0d917087166b71d3c2c83a39104b1/config.json HTTP/1.1\" 200 0\n",
      "2023-08-31 14:41:22,017 - DEBUG - https://huggingface.co:443 \"HEAD /bert-large-uncased-whole-word-masking/resolve/8b35f05561d0d917087166b71d3c2c83a39104b1/model.safetensors HTTP/1.1\" 302 0\n",
      "2023-08-31 14:41:22,194 - DEBUG - https://huggingface.co:443 \"HEAD /bert-large-uncased-whole-word-masking/resolve/8b35f05561d0d917087166b71d3c2c83a39104b1/pytorch_model.bin HTTP/1.1\" 302 0\n",
      "2023-08-31 14:41:22,390 - DEBUG - https://huggingface.co:443 \"HEAD /bert-large-uncased-whole-word-masking/resolve/8b35f05561d0d917087166b71d3c2c83a39104b1/tokenizer.json HTTP/1.1\" 200 0\n",
      "2023-08-31 14:41:22,598 - DEBUG - https://huggingface.co:443 \"HEAD /bert-large-uncased-whole-word-masking/resolve/8b35f05561d0d917087166b71d3c2c83a39104b1/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "2023-08-31 14:41:22,756 - DEBUG - https://huggingface.co:443 \"HEAD /bert-large-uncased-whole-word-masking/resolve/8b35f05561d0d917087166b71d3c2c83a39104b1/vocab.txt HTTP/1.1\" 200 0\n",
      "2023-08-31 14:41:22,756 - WARNING - No sentence-transformers model found with name C:\\Users\\thiag/.cache\\torch\\sentence_transformers\\bert-large-uncased-whole-word-masking. Creating a new one with MEAN pooling.\n",
      "2023-08-31 14:41:24,042 - INFO - Use pytorch device: cpu\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "new(): invalid data type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# bert-large-uncased-whole-word-masking\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# bert-base-multilingual-cased\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# 'all-mpnet-base-v2'\u001B[39;00m\n\u001B[0;32m      4\u001B[0m bert_sim \u001B[38;5;241m=\u001B[39m PatentSimilarity(df_patents\u001B[38;5;241m=\u001B[39mdf_thiago, model_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbert-large-uncased-whole-word-masking\u001B[39m\u001B[38;5;124m'\u001B[39m, does_include\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, does_not_include\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m----> 5\u001B[0m df_all_sim \u001B[38;5;241m=\u001B[39m \u001B[43mbert_sim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msimilarity_between_all_patents_with_all_subsetors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbreak_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[4], line 84\u001B[0m, in \u001B[0;36mPatentSimilarity.similarity_between_all_patents_with_all_subsetors\u001B[1;34m(self, break_index)\u001B[0m\n\u001B[0;32m     82\u001B[0m df_all \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame()\n\u001B[0;32m     83\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index, row \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf_patents\u001B[38;5;241m.\u001B[39miterrows():\n\u001B[1;32m---> 84\u001B[0m     row_sim \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msimilarity_between_one_patent_with_all_subsetors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrow\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpublication_number\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrow\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mabstract\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     85\u001B[0m     df_all \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([df_all, row_sim])\n\u001B[0;32m     86\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m break_index \u001B[38;5;241m==\u001B[39m index:\n",
      "Cell \u001B[1;32mIn[4], line 75\u001B[0m, in \u001B[0;36mPatentSimilarity.similarity_between_one_patent_with_all_subsetors\u001B[1;34m(self, publication_number, patent_abstract)\u001B[0m\n\u001B[0;32m     73\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m subsetor_name, embeddings \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msubsetors_embeddings\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m     74\u001B[0m     col_names\u001B[38;5;241m.\u001B[39mappend(subsetor_name)\n\u001B[1;32m---> 75\u001B[0m     specific_sim  \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msimilarity_between_one_patent_with_one_subsetor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpatent_abstract\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msubsetor_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     76\u001B[0m     patents_similarities\u001B[38;5;241m.\u001B[39mappend(specific_sim)\n\u001B[0;32m     78\u001B[0m df_one_patent \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame([patents_similarities], columns\u001B[38;5;241m=\u001B[39mcol_names)\n",
      "Cell \u001B[1;32mIn[4], line 64\u001B[0m, in \u001B[0;36mPatentSimilarity.similarity_between_one_patent_with_one_subsetor\u001B[1;34m(self, patent_abstract, subsetor_name)\u001B[0m\n\u001B[0;32m     61\u001B[0m similarity \u001B[38;5;241m=\u001B[39m util\u001B[38;5;241m.\u001B[39mcos_sim(patent_abstract_emb, subsetor_definition_emb)\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     63\u001B[0m top_k \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(\u001B[38;5;241m5\u001B[39m, \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msubsetors_definitions))\n\u001B[1;32m---> 64\u001B[0m cos_scores \u001B[38;5;241m=\u001B[39m \u001B[43mutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcos_sim\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_embedding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msubsetors_definitions\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     65\u001B[0m top_results \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtopk(cos_scores, k\u001B[38;5;241m=\u001B[39mtop_k)\n\u001B[0;32m     67\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m======================\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Genome\\venv\\lib\\site-packages\\sentence_transformers\\util.py:36\u001B[0m, in \u001B[0;36mcos_sim\u001B[1;34m(a, b)\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;124;03mComputes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\u001B[39;00m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;124;03m:return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\u001B[39;00m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(a, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m---> 36\u001B[0m     a \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(b, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[0;32m     39\u001B[0m     b \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(b)\n",
      "\u001B[1;31mTypeError\u001B[0m: new(): invalid data type 'str'"
     ]
    }
   ],
   "source": [
    "# bert-large-uncased-whole-word-masking\n",
    "# bert-base-multilingual-cased\n",
    "# 'all-mpnet-base-v2'\n",
    "bert_sim = PatentSimilarity(df_patents=df_thiago, model_name='bert-large-uncased-whole-word-masking', does_include=1, does_not_include=-1)\n",
    "df_all_sim = bert_sim.similarity_between_all_patents_with_all_subsetors(break_index=2)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "   publication_number                                    patent_abstract  \\\n0         20080017043  A brew stick for brewing a measure of brewable...   \n0         20080050603  Polylactide polymers are reacted with an epoxy...   \n0         20080007823  An interferometer comprises a light source uni...   \n\n  score_name  score_max  \n0     Agtech   0.780324  \n0     Agtech   0.802485  \n0     Agtech   0.759884  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>publication_number</th>\n      <th>patent_abstract</th>\n      <th>score_name</th>\n      <th>score_max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20080017043</td>\n      <td>A brew stick for brewing a measure of brewable...</td>\n      <td>Agtech</td>\n      <td>0.780324</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>20080050603</td>\n      <td>Polylactide polymers are reacted with an epoxy...</td>\n      <td>Agtech</td>\n      <td>0.802485</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>20080007823</td>\n      <td>An interferometer comprises a light source uni...</td>\n      <td>Agtech</td>\n      <td>0.759884</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_sim[['publication_number', 'patent_abstract', 'score_name', 'score_max']].head(50)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T22:54:41.081200700Z",
     "start_time": "2023-08-30T22:54:41.064067600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A brew stick for brewing a measure of brewable material. The brew stick may include a bag with the measure of brewable material positioned therein and a plate. The plate may include a pair of wings and a central tab. The bag is connected to the central tab such that the bag can be raised within the wings.\n",
      "\n",
      "Agtech\n",
      "0.7803242206573486\n",
      "\n",
      "\n",
      "Polylactide polymers are reacted with an epoxy-functional acrylate polymer to introduce long-chain branching into the polymer. The acrylate polymer provides a flexible means for introducing a controllable amount of branching into the polylactide polymer, with little risk of forming gelled or highly crosslinked structures. The branched polylactide polymers have excellent melt rheological properties that make them more easily processable in various melt-processing applications.\n",
      "\n",
      "Agtech\n",
      "0.802484929561615\n",
      "\n",
      "\n",
      "An interferometer comprises a light source unit, a first splitter, a reference beam unit and a detection unit. The light source unit provides a laser beam. The first splitter receives the laser beam from the light source unit and splits the laser beam into a first beam and a second beam. The reference beam unit comprises a frequency shifter, a stopper and a spherical mirror. A center of the frequency shifter is located on a curvature center of the spherical mirror, the first beam traveling from the first splitter to the frequency shifter, the frequency shifter splitting the first beam into a diffraction beam and a zero-order beam, wherein the diffraction beam travels to the spherical mirror, reflected by the spherical mirror toward the frequency shifter, passing the frequency shifter to become a reference beam, and the zero-order beam is stopped by the stopper. The detection unit receives the reference beam from the reference beam unit.\n",
      "\n",
      "Agtech\n",
      "0.7598839402198792\n"
     ]
    }
   ],
   "source": [
    "for index, row in df_all_sim.iterrows():\n",
    "    print(f'{row[\"patent_abstract\"]}\\n\\n{row[\"score_name\"]}\\n{row[\"score_max\"]}\\n\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T22:54:42.193070600Z",
     "start_time": "2023-08-30T22:54:42.183049600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
