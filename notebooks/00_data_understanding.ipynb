{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color='#D0533E'><b>Startup Genome</b></font> - <font color='#0D0D2F'><b>Grupo 3</b></font></center>\n",
    "\n",
    "<center><font color='#02231c'><b>Módulo 1</font></b></center>\n",
    "<p><center>\n",
    "<table>\n",
    "  <tr>\n",
    "    <th><b>Nome</b></th>\n",
    "    <th><b>Contato</b></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Fulano1</td>\n",
    "    <td><a href=\"https://www.linkedin.com/in/leo-koki-shashiki/\">Linkedin</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Rafael Costa</td>\n",
    "    <td><a href=\"https://www.linkedin.com/in/rafael-costa-a642752b/\">Linkedin</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Thiago Seronni Mendonça</td>\n",
    "    <td><a href=\"https://www.linkedin.com/in/thiagoseronni/\">Linkedin</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Leo Koki Shashiki</td>\n",
    "    <td><a href=\"https://www.linkedin.com/in/leo-koki-shashiki/\">Linkedin</a></td>\n",
    "  </tr>\n",
    "</font></table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Startup Genome’s Global Startup Ecosystem Report (GSER) is powered by the world’s most comprehensive and quality controlled dataset on startup ecosystems. Informed by information on 3.5 million startups across 290 global ecosystems, our data and insights are the product of over a decade of independent research and policy work.\n",
    "\n",
    "GSER 2023 ranks the top 30 and 10 runner-up global ecosystems, and includes a top 100 ranking of emerging ecosystems. It also takes a look at startup communities from a regional perspective, separately ranking ecosystems in Africa, Asia, Europe, Latin America, MENA, North America, and Oceania."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referências: https://www.ibm.com/docs/en/spss-modeler/saas?topic=data-writing-description-report\n",
    "\n",
    "Miro:   https://miro.com/app/board/uXjVMC_icTM=/ \\\n",
    "Trello: https://trello.com/b/bjkRHTjw/sirius-proj-1 \\\n",
    "GitHub: https://github.com/joaomorossini/startup_genome"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIBLIOTECAS UTILIZADAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-25T13:27:23.299727700Z",
     "start_time": "2023-06-25T13:27:22.910366Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-25T13:27:23.315415500Z",
     "start_time": "2023-06-25T13:27:23.299727700Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.utils.UsefulPaths import Paths\n",
    "paths = Paths()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-25T13:27:27.437510200Z",
     "start_time": "2023-06-25T13:27:23.315415500Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m df_abstract \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_parquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpaths\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraw_parquet_abstract\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m df_abstract\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Genome\\venv\\lib\\site-packages\\pandas\\io\\parquet.py:509\u001B[0m, in \u001B[0;36mread_parquet\u001B[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, **kwargs)\u001B[0m\n\u001B[0;32m    506\u001B[0m     use_nullable_dtypes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    507\u001B[0m check_dtype_backend(dtype_backend)\n\u001B[1;32m--> 509\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m impl\u001B[38;5;241m.\u001B[39mread(\n\u001B[0;32m    510\u001B[0m     path,\n\u001B[0;32m    511\u001B[0m     columns\u001B[38;5;241m=\u001B[39mcolumns,\n\u001B[0;32m    512\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[0;32m    513\u001B[0m     use_nullable_dtypes\u001B[38;5;241m=\u001B[39muse_nullable_dtypes,\n\u001B[0;32m    514\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m    515\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    516\u001B[0m )\n",
      "File \u001B[1;32m~\\PycharmProjects\\Genome\\venv\\lib\\site-packages\\pandas\\io\\parquet.py:227\u001B[0m, in \u001B[0;36mPyArrowImpl.read\u001B[1;34m(self, path, columns, use_nullable_dtypes, dtype_backend, storage_options, **kwargs)\u001B[0m\n\u001B[0;32m    220\u001B[0m path_or_handle, handles, kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfilesystem\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m _get_path_or_handle(\n\u001B[0;32m    221\u001B[0m     path,\n\u001B[0;32m    222\u001B[0m     kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfilesystem\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m    223\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[0;32m    224\u001B[0m     mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    225\u001B[0m )\n\u001B[0;32m    226\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 227\u001B[0m     pa_table \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi\u001B[38;5;241m.\u001B[39mparquet\u001B[38;5;241m.\u001B[39mread_table(\n\u001B[0;32m    228\u001B[0m         path_or_handle, columns\u001B[38;5;241m=\u001B[39mcolumns, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    229\u001B[0m     )\n\u001B[0;32m    230\u001B[0m     result \u001B[38;5;241m=\u001B[39m pa_table\u001B[38;5;241m.\u001B[39mto_pandas(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mto_pandas_kwargs)\n\u001B[0;32m    232\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m manager \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\Genome\\venv\\lib\\site-packages\\pyarrow\\parquet\\core.py:2986\u001B[0m, in \u001B[0;36mread_table\u001B[1;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit)\u001B[0m\n\u001B[0;32m   2975\u001B[0m         \u001B[38;5;66;03m# TODO test that source is not a directory or a list\u001B[39;00m\n\u001B[0;32m   2976\u001B[0m         dataset \u001B[38;5;241m=\u001B[39m ParquetFile(\n\u001B[0;32m   2977\u001B[0m             source, metadata\u001B[38;5;241m=\u001B[39mmetadata, read_dictionary\u001B[38;5;241m=\u001B[39mread_dictionary,\n\u001B[0;32m   2978\u001B[0m             memory_map\u001B[38;5;241m=\u001B[39mmemory_map, buffer_size\u001B[38;5;241m=\u001B[39mbuffer_size,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2983\u001B[0m             thrift_container_size_limit\u001B[38;5;241m=\u001B[39mthrift_container_size_limit,\n\u001B[0;32m   2984\u001B[0m         )\n\u001B[1;32m-> 2986\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_threads\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_threads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2987\u001B[0m \u001B[43m                        \u001B[49m\u001B[43muse_pandas_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_pandas_metadata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2989\u001B[0m warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   2990\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPassing \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muse_legacy_dataset=True\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m to get the legacy behaviour is \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2991\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdeprecated as of pyarrow 8.0.0, and the legacy implementation will \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2992\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbe removed in a future version.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   2993\u001B[0m     \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m   2995\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ignore_prefixes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\Genome\\venv\\lib\\site-packages\\pyarrow\\parquet\\core.py:2614\u001B[0m, in \u001B[0;36m_ParquetDatasetV2.read\u001B[1;34m(self, columns, use_threads, use_pandas_metadata)\u001B[0m\n\u001B[0;32m   2606\u001B[0m         index_columns \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m   2607\u001B[0m             col \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m _get_pandas_index_columns(metadata)\n\u001B[0;32m   2608\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(col, \u001B[38;5;28mdict\u001B[39m)\n\u001B[0;32m   2609\u001B[0m         ]\n\u001B[0;32m   2610\u001B[0m         columns \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2611\u001B[0m             \u001B[38;5;28mlist\u001B[39m(columns) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mset\u001B[39m(index_columns) \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mset\u001B[39m(columns))\n\u001B[0;32m   2612\u001B[0m         )\n\u001B[1;32m-> 2614\u001B[0m table \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_table\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2615\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mfilter\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_filter_expression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2616\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_threads\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_threads\u001B[49m\n\u001B[0;32m   2617\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2619\u001B[0m \u001B[38;5;66;03m# if use_pandas_metadata, restore the pandas metadata (which gets\u001B[39;00m\n\u001B[0;32m   2620\u001B[0m \u001B[38;5;66;03m# lost if doing a specific `columns` selection in to_table)\u001B[39;00m\n\u001B[0;32m   2621\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_pandas_metadata:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "df_abstract = pd.read_parquet(paths.raw_parquet_abstract)\n",
    "df_abstract.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ipc = pd.read_excel(paths.raw_ipc_titles)\n",
    "df_ipc.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_of_companies = pd.read_csv(paths.raw_list_of_companies)\n",
    "df_list_of_companies.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_patents = pd.read_parquet(paths.raw_parquet_raw_patents)\n",
    "df_raw_patents.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table_for_applicants = pd.read_csv(filepath_or_buffer=paths.raw_table_for_applicants, header=None, names=['patent_id','owner','city_region','country','year'])\n",
    "df_table_for_applicants.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - What is the format of the data?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is in CSV and XLSX format, but the files larger than 1GB were converted to Parquet. This improved reading performance and reduced storage space usage."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Identify the method used to capture the data--for example, ODBC."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data provided was in csv and xlsx extension and we are using pandas library to read it.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - How large is the database (in numbers of rows and columns)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Dataset abstract.parquet         |  Row: {df_abstract.shape[0]}\\t Columns: {df_abstract.shape[1]}')\n",
    "print(f'Dataset IPC Titles.xlsx          |  Row: {df_ipc.shape[0]}\\t\\t Columns: {df_ipc.shape[1]}')\n",
    "print(f'Dataset ListOfCompanies.csv      |  Row: {df_list_of_companies.shape[0]}\\t Columns: {df_list_of_companies.shape[1]}')\n",
    "print(f'Dataset raw_patents.parquet      |  Row: {df_raw_patents.shape[0]}\\t Columns: {df_raw_patents.shape[1]}')\n",
    "print(f'Dataset table_for_applicants.csv |  Row: {df_table_for_applicants.shape[0]}\\t Columns: {df_table_for_applicants.shape[1]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df = [df_abstract, df_ipc, df_list_of_companies, df_raw_patents, df_table_for_applicants]\n",
    "dict_df = {\n",
    "    'abst': 'Abstract',\n",
    "    'ipc': 'IPC Titles',\n",
    "    'comp': 'List of Companies',\n",
    "    'pat': 'Raw Patents',\n",
    "    'app': 'Table for Applicants'\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise de Nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_list = []\n",
    "for df, values in zip(list_df, dict_df.values()):\n",
    "    df_null_percentage = pd.DataFrame(df.isnull().sum()* 100 / len(df)).reset_index()\n",
    "    df_null_percentage.rename(columns={'index': values, 0:'null_percentage'}, inplace = True)\n",
    "\n",
    "    df_null_percentage.null_percentage = df_null_percentage.null_percentage.round(1)\n",
    "    null_list.append(df_null_percentage)\n",
    "    print('#'*100 + '\\n')\n",
    "    print(values+ '\\n')\n",
    "    display(df_null_percentage.sort_values(by = 'null_percentage', ascending=False))\n",
    "    print('_'*100 + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise de Duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_list = []\n",
    "for df, values in zip(list_df, dict_df.values()):\n",
    "    df_duplicated = pd.DataFrame(df.duplicated().value_counts()).reset_index()\n",
    "    df_duplicated.rename(columns={'index': 'duplicated', 0: values}, inplace = True)\n",
    "    duplicated_list.append(df_duplicated)\n",
    "\n",
    "    print('#'*100 + '\\n')\n",
    "    print(values+ '\\n')\n",
    "    display(df_duplicated)\n",
    "    print('_'*100 + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Does the data include characteristics relevant to the business question?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently they have all data needed to answer the questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5 - What data types are present (symbolic, numeric, etc.)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dtypes = []\n",
    "for df, values in zip(list_df, dict_df.values()):\n",
    "\n",
    "    df_dtypes = pd.DataFrame(df.dtypes).reset_index()\n",
    "    df_dtypes.rename(columns={'index': values, 0:'data_type'}, inplace=True )\n",
    "    list_dtypes.append(df_dtypes)\n",
    "\n",
    "    print('#'*100 + '\\n')\n",
    "    print(values+ '\\n')\n",
    "    display(df_dtypes)\n",
    "    print('_'*100 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unique_patents_abstract = len(df_abstract['publication_number'].unique())\n",
    "print(f'Unique patents for abstract: {unique_patents_abstract}')\n",
    "\n",
    "unique_patents_raw_patents = len(df_raw_patents['patent_id'].unique())\n",
    "print(f'Unique patents for raw_patents: {unique_patents_raw_patents}')\n",
    "\n",
    "unique_patents_applicants = len(df_table_for_applicants['patent_id'].unique())\n",
    "print(f'Unique patents for applicants: {unique_patents_applicants}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df_abstract[~df_abstract['publication_number'].isin(df_raw_patents['patent_id'])]\n",
    "# df_abstract"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Did you compute basic statistics for the key attributes? What insight did this provide into the business question?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Are you able to prioritize relevant attributes? If not, are business analysts available to provide further insight?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - What sort of hypotheses have you formed about the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 - Which attributes seem promising for further analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 - Have your explorations revealed new characteristics about the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 - How have these explorations changed your initial hypothesis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 - Can you identify particular subsets of data for later use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13 - Take another look at your data mining goals. Has this exploration altered the goals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
